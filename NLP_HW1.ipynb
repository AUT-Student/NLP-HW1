{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNb3b+5ciOU5eL9DVMmmh7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW1/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "e6rp27qwNwJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "vJnzzUYZ1BoE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "18SpJaECNuHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSRMQW5y0GbV",
        "outputId": "6fd2949b-22f8-4e65-e496-e104f2fde9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
            "To: /content/HW1-datasets.zip\n",
            "\r  0% 0.00/3.14M [00:00<?, ?B/s]\r100% 3.14M/3.14M [00:00<00:00, 125MB/s]\n",
            "Archive:  /content/HW1-datasets.zip\n",
            "   creating: content/HW1-datasets/\n",
            "  inflating: content/HW1-datasets/train.txt  \n",
            "  inflating: content/HW1-datasets/valid.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete.txt  \n",
            "  inflating: content/HW1-datasets/test.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete_gold.txt  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
        "!unzip /content/HW1-datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Language Model"
      ],
      "metadata": {
        "id": "ZfGWWpPEN-aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting"
      ],
      "metadata": {
        "id": "50IxVNrjOw1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "word_counter = {}\n",
        "pair_counter = {}\n",
        "vocabulary = set()\n",
        "N = 0\n",
        "\n",
        "for line in file.readlines():\n",
        "    words = line.split()\n",
        "    N += len(words)\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        vocabulary.add(word)\n",
        "        if word_counter.get(word) is None:\n",
        "            word_counter[word] = 0\n",
        "        word_counter[word] += 1\n",
        "    \n",
        "        if i>0:\n",
        "            pair = (words[i-1], word)\n",
        "            if pair_counter.get(pair) is None:\n",
        "                pair_counter[pair] = 0\n",
        "            pair_counter[pair] += 1"
      ],
      "metadata": {
        "id": "XZfxUo0POQB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "pl61UXSjO0WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unigram(delta):\n",
        "  def unigram(word):\n",
        "    if word in vocabulary:\n",
        "      return word_counter[word] / N \n",
        "    else:\n",
        "      return delta / N\n",
        "\n",
        "  return unigram"
      ],
      "metadata": {
        "id": "HoGMfsB3OSns"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bigram(delta, unigram):\n",
        "    def calculate_B(delta):\n",
        "        B_word = {}\n",
        "\n",
        "        for word in vocabulary:\n",
        "          B_word[word] = 0\n",
        "\n",
        "        for pair in pair_counter:\n",
        "            word1 = pair[0]\n",
        "            B_word[word1] += 1\n",
        "        \n",
        "        for word in vocabulary:\n",
        "          if B_word[word]==0:\n",
        "            B_word[word] = 1\n",
        "\n",
        "        return B_word\n",
        "    \n",
        "    B_word = calculate_B(delta)\n",
        "    \n",
        "    def bigram(word1, word2):\n",
        "      p_bg = unigram(word2)\n",
        "      if word1 not in vocabulary:\n",
        "        return p_bg\n",
        "      else:\n",
        "        alpha = delta / word_counter[word1] * B_word[word1]\n",
        "\n",
        "        return (max(pair_counter.get((word1, word2), 0) - delta, 0) / word_counter[word1]) + alpha * p_bg\n",
        "\n",
        "    return bigram"
      ],
      "metadata": {
        "id": "JrmBTxHTOVQc"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_statistical_dataset(path, number_words, line_limit=None, data_number_limit=None):\n",
        "  file = open(path)\n",
        "\n",
        "  X_dataset = []\n",
        "  Y_dataset = []\n",
        "\n",
        "  for i, line in enumerate(file.readlines()):\n",
        "    if line_limit is not None and i >= line_limit:\n",
        "      break\n",
        "\n",
        "    all_words = line.split()\n",
        "\n",
        "    for j in range(number_words, len(all_words)):\n",
        "      words = []\n",
        "      for k in range(number_words):\n",
        "        word = all_words[j-k-1] \n",
        "        words.append(word)\n",
        "      \n",
        "      X_dataset.append(words)\n",
        "\n",
        "      word_target = all_words[j]\n",
        "      Y_dataset.append(word_target)\n",
        "\n",
        "  print(f\"Maximum Data = {len(X_dataset)}\")\n",
        "\n",
        "  if data_number_limit is not None:\n",
        "    random.seed(0)\n",
        "    X_dataset, Y_dataset = zip(*random.sample(list(zip(X_dataset, Y_dataset)), data_number_limit))\n",
        "\n",
        "  X_dataset = np.array(X_dataset)\n",
        "  Y_dataset = np.array(Y_dataset)\n",
        "  return X_dataset, Y_dataset"
      ],
      "metadata": {
        "id": "A31fLY_2oq5G"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity"
      ],
      "metadata": {
        "id": "bwlvnRgmO8px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_perplexity(path):\n",
        "  file = open(path)\n",
        "\n",
        "  counter = 0\n",
        "  probability = 0\n",
        "\n",
        "  for line in file.readlines():\n",
        "    words = line.split()\n",
        "    \n",
        "    for word in words:\n",
        "      probability += math.log(unigram(word))\n",
        "      counter += 1\n",
        "\n",
        "  log_perplexity = -1/counter*probability\n",
        "  \n",
        "  perplexity = math.exp(log_perplexity)\n",
        "\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "e40Yl-sghFhm"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_perplexity(path):\n",
        "  file = open(path)\n",
        "\n",
        "  counter = 0\n",
        "  probability = 0\n",
        "\n",
        "  for line in file.readlines():\n",
        "    words = line.split()\n",
        "  \n",
        "    for i in range(1, len(words)):\n",
        "      probability += math.log(bigram(words[i-1], words[i]))\n",
        "      counter += 1\n",
        "\n",
        "  log_perplexity = -1/counter*probability  \n",
        "\n",
        "  perplexity = math.exp(log_perplexity)\n",
        "\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "bm8eMaNQok7Z"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune"
      ],
      "metadata": {
        "id": "DM5pqRcEYagN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/content/HW1-datasets/valid.txt\"\n",
        "\n",
        "best_unigram_perplexity = np.inf\n",
        "best_unigram_delta = None\n",
        "\n",
        "for delta in np.arange(0.2, 0.98, 0.02):\n",
        "  unigram = create_unigram(delta=delta)\n",
        "\n",
        "  new_unigram_perplexity = unigram_perplexity(path)\n",
        "\n",
        "  if new_unigram_perplexity < best_unigram_perplexity:\n",
        "    best_unigram_perplexity = new_unigram_perplexity\n",
        "    best_unigram_delta = delta\n",
        "\n",
        "print(f\"Delta = {best_unigram_delta} Perplexity = {best_unigram_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPks11PhuQrC",
        "outputId": "4dc12491-f48b-4454-b73b-c14fccf7a083"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta = 0.9599999999999995 Perplexity = 1828.493440423088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/content/HW1-datasets/valid.txt\"\n",
        "\n",
        "best_bigram_perplexity = np.inf\n",
        "best_bigram_delta = None\n",
        "\n",
        "for delta in np.arange(0.2, 0.98, 0.02):\n",
        "  unigram = create_unigram(delta=0.96)\n",
        "  bigram = create_bigram(delta=delta, unigram=unigram)\n",
        "\n",
        "  new_bigram_perplexity = bigram_perplexity(path)\n",
        "\n",
        "  if new_bigram_perplexity < best_bigram_perplexity:\n",
        "    best_bigram_perplexity = new_bigram_perplexity\n",
        "    best_bigram_delta = delta\n",
        "\n",
        "print(f\"Delta = {best_bigram_delta} Perplexity = {best_bigram_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b0gW7l0qfdc",
        "outputId": "10b7b257-3e9c-4d36-8bf7-f84040c092fc"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta = 0.8799999999999997 Perplexity = 1318.4189504258493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram = create_unigram(delta=0.96)\n",
        "bigram = create_bigram(delta=0.88, unigram=unigram)"
      ],
      "metadata": {
        "id": "4O5V1R7iOXJm"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in [\"train\", \"test\", \"valid\"]:\n",
        "  path = f\"/content/content/HW1-datasets/{text}.txt\"\n",
        "  print(f\"Unigram perplexity of {text} = {unigram_perplexity(path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy4igi1BpC33",
        "outputId": "81b0c13a-f7d2-43cc-a44c-12e16bee5e05"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram perplexity of train = 1768.173187352548\n",
            "Unigram perplexity of test = 1810.0517762463387\n",
            "Unigram perplexity of valid = 1828.493440423088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in [\"train\", \"test\", \"valid\"]:\n",
        "  path = f\"/content/content/HW1-datasets/{text}.txt\"\n",
        "  print(f\"Bigram perplexity of {text} = {bigram_perplexity(path)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiM_PsJjpFWo",
        "outputId": "b9d135fa-fdf0-4089-f7f8-f29580c8a48e"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram perplexity of train = 322.6322649838004\n",
            "Bigram perplexity of test = 1326.7163336443393\n",
            "Bigram perplexity of valid = 1318.4189504258493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram"
      ],
      "metadata": {
        "id": "tAPzVoltPGku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Test"
      ],
      "metadata": {
        "id": "SG6A1O_RoHmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, Y_test = create_statistical_dataset(path=\"/content/content/HW1-datasets/test.txt\",\n",
        "                                            number_words=0,\n",
        "                                            data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjAVwcQxESbL",
        "outputId": "1b43b1ea-6806-4292-e9b4-a8703c3d95aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 85324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_word = None\n",
        "best_probability = -math.inf\n",
        "for condidate_word in vocabulary:\n",
        "    if unigram(condidate_word) > best_probability:\n",
        "        best_probability = unigram(condidate_word)\n",
        "        best_word = condidate_word\n",
        "\n",
        "Y_test_predict = []\n",
        "for _ in range(len(Y_test)):\n",
        "  Y_test_predict.append(best_word)\n",
        "\n",
        "print(f\"Test Accuracy = {accuracy_score(Y_test, Y_test_predict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ncFpRn2Fq-e",
        "outputId": "ee078802-7a1d-45ef-8332-77781e0b5d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 0.03968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Test"
      ],
      "metadata": {
        "id": "4sjzKpxWoAhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_word = None\n",
        "best_probability = -math.inf\n",
        "for condidate_word in vocabulary:\n",
        "    if unigram(condidate_word) > best_probability:\n",
        "        best_probability = unigram(condidate_word)\n",
        "        best_word = condidate_word\n",
        "\n",
        "file = open(\"/content/content/HW1-datasets/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"/content/content/HW1-datasets/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    print(line.strip(), end=\" \")\n",
        "    for _ in range(digit):\n",
        "        print(f\"\\x1b[32m{best_word}\\x1b[0m\", end= \" \")\n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2BEpQXNPKqS",
        "outputId": "744c0eee-3291-44f9-b062-4ab7ab4c13a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "این سخن حقست اگر نزد سخن گستر \u001b[32mو\u001b[0m \n",
            "این سخن حقست اگر نزد سخن گستر برند\n",
            "\n",
            "آنکه با یوسف صدیق چنین خواهد \u001b[32mو\u001b[0m \n",
            "آنکه با یوسف صدیق چنین خواهد کرد\n",
            "\n",
            "هیچ دانی چکند صحبت او با \u001b[32mو\u001b[0m \n",
            "هیچ دانی چکند صحبت او با دگران\n",
            "\n",
            "سرمه دهی بصر بری سخت خوش است \u001b[32mو\u001b[0m \n",
            "سرمه دهی بصر بری سخت خوش است تاجری\n",
            "\n",
            "آتش ابراهیم را \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "آتش ابراهیم را نبود زیان\n",
            "\n",
            "من که اندر سر \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "من که اندر سر جنونی داشتم\n",
            "\n",
            "هر شیر شرزه را که به نیش \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "هر شیر شرزه را که به نیش سنان گزید\n",
            "\n",
            "هرکه از حق به \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "هرکه از حق به سوی او نظریست\n",
            "\n",
            "گفت این از \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "گفت این از خدای باید خواست\n",
            "\n",
            "کلاه لاله که لعل است \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \n",
            "کلاه لاله که لعل است اگر تو بشناسی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram"
      ],
      "metadata": {
        "id": "mEguCcW1PLz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Test"
      ],
      "metadata": {
        "id": "boAS54xPoRI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, Y_test = create_statistical_dataset(path=\"/content/content/HW1-datasets/test.txt\",\n",
        "                                            number_words=1,\n",
        "                                            data_number_limit=10*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsM8Xk2DG9wd",
        "outputId": "14fe8759-0501-47cc-8662-65b838c7b5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 73199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_predict = []\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if i%1000 == 0:\n",
        "    print(f\"i = {i}\")\n",
        "\n",
        "  Y_test[i]\n",
        "  best_word = None\n",
        "  best_probability = -math.inf\n",
        "  \n",
        "  for condidate_word in vocabulary:\n",
        "      try:\n",
        "        if bigram(X_test[i][0], condidate_word) > best_probability:\n",
        "            best_probability = bigram(last_word, condidate_word)\n",
        "            best_word = condidate_word\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "  Y_test_predict.append(best_word)\n",
        "\n",
        "print(f\"Test Accuracy = {accuracy_score(Y_test, Y_test_predict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "SYN7f4DEGsao",
        "outputId": "567a4b16-af38-4aeb-804b-72c47f29e7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "i = 1000\n",
            "i = 2000\n",
            "i = 3000\n",
            "i = 4000\n",
            "i = 5000\n",
            "i = 6000\n",
            "i = 7000\n",
            "i = 8000\n",
            "i = 9000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-2aa5b706f098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mY_test_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy = {accuracy_score(Y_test, Y_test_predict)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m  \u001b[0;31m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_predict_new = []\n",
        "\n",
        "for p in Y_test_predict:\n",
        "  if p is None:\n",
        "    Y_test_predict_new.append(\"\")\n",
        "  else:\n",
        "    Y_test_predict_new.append(p)\n",
        "\n",
        "print(f\"Test Accuracy = {accuracy_score(Y_test, Y_test_predict_new)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yteUJvpN5HH",
        "outputId": "68520b1f-5c23-492e-b21b-f32ae22a4ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 0.0518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Test"
      ],
      "metadata": {
        "id": "32uyvQtKoU9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"/content/content/HW1-datasets/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    line = line.strip()\n",
        "    words = line.split()\n",
        "    \n",
        "    last_word = words[-1]\n",
        "    \n",
        "    print(line, end=\" \")\n",
        "    \n",
        "    for _ in range(digit):        \n",
        "        best_word = None\n",
        "        best_probability = -math.inf\n",
        "        \n",
        "        for condidate_word in vocabulary:\n",
        "            if bigram(last_word, condidate_word) > best_probability:\n",
        "                best_probability = bigram(last_word, condidate_word)\n",
        "                best_word = condidate_word\n",
        "            \n",
        "        print(f\"\\x1b[32m{best_word}\\x1b[0m\", end= \" \")\n",
        "        last_word = best_word\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])  "
      ],
      "metadata": {
        "id": "3lhK_uBpPPzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Language Model"
      ],
      "metadata": {
        "id": "mE8eCJIqNo4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary"
      ],
      "metadata": {
        "id": "73VIJ1hZPsJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "vocabulary = set()\n",
        "\n",
        "for i, line in enumerate(file.readlines()):\n",
        "  if i < 30*1000:\n",
        "    words = line.split()\n",
        "\n",
        "    for word in words:\n",
        "      vocabulary.add(word)\n",
        "\n",
        "vocabulary = sorted(vocabulary)\n",
        "print(f\"Vocabulary Size = {len(vocabulary)}\")"
      ],
      "metadata": {
        "id": "hiuj1h6ZcmhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5408ea0-e63f-4892-9afa-8138e2c18348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size = 21188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Uh2LXDOjPwss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bisect import bisect_left\n",
        "\n",
        "class NLM(keras.Model):\n",
        "  def __init__(self, vocabulary_size, input_size, embedding_size=128, hidden_size=256):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.model = keras.models.Sequential([\n",
        "                                          keras.layers.Input((input_size,), name=\"Input\"),\n",
        "                                          keras.layers.Embedding(vocabulary_size + 1, embedding_size, name=\"Embedding\"),\n",
        "                                          keras.layers.Flatten(name=\"Flatten\"),\n",
        "                                          keras.layers.Dense(units=hidden_size, name=\"Hidden\"),\n",
        "                                          keras.layers.Dense(units=vocabulary_size, activation=\"softmax\", name=\"Output\"),\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def single_predict(self, words):\n",
        "    indexes = []\n",
        "    for word in words:\n",
        "      index = convert_word_to_index(word)\n",
        "      indexes.append(index)\n",
        "    \n",
        "    indexes = tf.constant(indexes)\n",
        "    indexes = tf.expand_dims(indexes, 0)\n",
        "\n",
        "    model_output =  self.model(indexes)\n",
        "\n",
        "    predicted_word_indexes = keras.backend.argmax(model_output, axis=1)\n",
        "    predicted_word_index = tf.squeeze(predicted_word_indexes) \n",
        "    predicted_word = convert_index_to_word(predicted_word_index)\n",
        "    return predicted_word\n",
        "\n",
        "  def perplexity(self, path):\n",
        "    file = open(path)\n",
        "\n",
        "    sum_perplexity = 0\n",
        "    number_data = 0\n",
        "    for line in file.readlines():\n",
        "      number_data += 1\n",
        "      words = line.split()\n",
        "\n",
        "      probability = 1\n",
        "      for i in range(self.input_size, len(words)):\n",
        "        target_word = words[i]\n",
        "\n",
        "        for j in range(self.input_size):\n",
        "          words[i-j-1]\n",
        "\n",
        "      ############################################################\n",
        "\n",
        "\n",
        "      for i, word in enumerate(self.input_size, words):\n",
        "        probability *= unigram(word)\n",
        "\n",
        "      inv_probability = 1/probability\n",
        "      perplexity = inv_probability ** (1/len(words))\n",
        "      sum_perplexity += perplexity\n",
        "    \n",
        "    avg_perplexity = sum_perplexity / number_data\n",
        "\n",
        "    return avg_perplexity"
      ],
      "metadata": {
        "id": "e77K_UbOeJZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliry Functions"
      ],
      "metadata": {
        "id": "yxJ9okWyRqsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_word_to_index(word):\n",
        "  pos = bisect_left(vocabulary, word, 0, len(vocabulary))\n",
        "  return pos if pos != len(vocabulary) and vocabulary[pos] == word else len(vocabulary)"
      ],
      "metadata": {
        "id": "ZOhhnodXRn_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_index_to_word(index):\n",
        "  return vocabulary[index]"
      ],
      "metadata": {
        "id": "vI8fSzGIR3WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neural_dataset(path, number_words, line_limit=None, data_number_limit=None):\n",
        "  file = open(path)\n",
        "\n",
        "  X_dataset = []\n",
        "  Y_dataset = []\n",
        "\n",
        "  for i, line in enumerate(file.readlines()):\n",
        "    if line_limit is not None and i >= line_limit:\n",
        "      break\n",
        "\n",
        "    words = line.split()\n",
        "\n",
        "    for j in range(number_words, len(words)):\n",
        "      indexes = []\n",
        "      for k in range(number_words):\n",
        "        word = words[j-k-1]\n",
        "        index = convert_word_to_index(word) \n",
        "        indexes.append(index)\n",
        "      \n",
        "      X_dataset.append(indexes)\n",
        "\n",
        "      word_target = words[j]\n",
        "      index_target = convert_word_to_index(word_target)\n",
        "      Y_dataset.append(index_target)\n",
        "\n",
        "  print(f\"Maximum Data = {len(X_dataset)}\")\n",
        "  \n",
        "  if data_number_limit is not None:\n",
        "    random.seed(0)\n",
        "    X_dataset, Y_dataset = zip(*random.sample(list(zip(X_dataset, Y_dataset)), data_number_limit))\n",
        "\n",
        "  X_dataset = np.array(X_dataset)\n",
        "  Y_dataset = tf.one_hot(indices=Y_dataset, depth=len(vocabulary)).numpy()\n",
        "  return X_dataset, Y_dataset"
      ],
      "metadata": {
        "id": "ucaZ0YAnR44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity"
      ],
      "metadata": {
        "id": "5pqZZ1ulv7cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_perplexity(path):\n",
        "  file = open(path)\n",
        "\n",
        "  sum_perplexity = 0\n",
        "  number_data = 0\n",
        "  for line in file.readlines():\n",
        "    number_data += 1\n",
        "    words = line.split()\n",
        "\n",
        "    probability = 1\n",
        "    for i, word in enumerate(words):\n",
        "      probability *= unigram(word)\n",
        "\n",
        "    inv_probability = 1/probability\n",
        "    perplexity = inv_probability ** (1/len(words))\n",
        "    sum_perplexity += perplexity\n",
        "  \n",
        "  avg_perplexity = sum_perplexity / number_data\n",
        "\n",
        "  return avg_perplexity"
      ],
      "metadata": {
        "id": "3VhLwN7mv9Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram"
      ],
      "metadata": {
        "id": "4jMD3UatSo0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craete and Train"
      ],
      "metadata": {
        "id": "Z0Nhsam6YsFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_nlm = NLM(vocabulary_size=len(vocabulary), input_size=1)"
      ],
      "metadata": {
        "id": "O-LXOt3iSrnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_nlm.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "CNWnHdIhSWET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_neural_dataset(path=\"/content/content/HW1-datasets/train.txt\",\n",
        "                                         number_words=1,\n",
        "                                         line_limit=30*1000,\n",
        "                                         data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igR-h-a0WtnQ",
        "outputId": "ce6da7b1-b453-4059-9149-ef88fab1a626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 181278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, Y_valid = create_neural_dataset(path=\"/content/content/HW1-datasets/valid.txt\",\n",
        "                                         number_words=1,\n",
        "                                         data_number_limit=25*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdwYq8Vdfwuk",
        "outputId": "4baa7b9f-072d-4523-c656-1657c5151eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 114079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", restore_best_weights=True, patience=2)"
      ],
      "metadata": {
        "id": "ZdMHNi9rWTWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_nlm.fit(x=X_train, y=Y_train, validation_data= (X_valid, Y_valid), epochs=10, callbacks=[es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOfT191CNuAZ",
        "outputId": "478bf9a7-59b8-4cc5-dae6-14e17e0c2c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 109s 69ms/step - loss: 7.7940 - accuracy: 0.0509 - val_loss: 6.8476 - val_accuracy: 0.0540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2be044390>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Test"
      ],
      "metadata": {
        "id": "tONcfWFZYw9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Accuracy = {bigram_nlm.evaluate(x=X_train, y=Y_train, verbose=1)[1]}\")\n",
        "print(f\"Valid Accuracy = {bigram_nlm.evaluate(x=X_valid, y=Y_valid, verbose=1)[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWMYBvWxXU0X",
        "outputId": "263bcd0b-d3a6-418c-a31a-0cfea1d39a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 51s 33ms/step - loss: 7.0800 - accuracy: 0.0584\n",
            "Train Accuracy = 0.05835999920964241\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 6.8476 - accuracy: 0.0540\n",
            "Valid Accuracy = 0.05400000140070915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train, Y_train, X_valid, Y_valid"
      ],
      "metadata": {
        "id": "Qojcb_AqXGGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, Y_test = create_neural_dataset(path=\"/content/content/HW1-datasets/test.txt\",\n",
        "                                       number_words=1,\n",
        "                                       data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz3vTu7IibI-",
        "outputId": "0e883d47-3aff-4ab7-f1c8-bb4278004c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 73199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Accuracy = {bigram_nlm.evaluate(x=X_test, y=Y_test, verbose=1)[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0zIuyUtXLYU",
        "outputId": "c299056e-7dcc-4cd5-96f8-0ee32778290c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 82s 52ms/step - loss: 6.8664 - accuracy: 0.0528\n",
            "Test Accuracy = 0.05283999815583229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_test, Y_test"
      ],
      "metadata": {
        "id": "_qLKTR14YUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Test"
      ],
      "metadata": {
        "id": "CGLfy2UuZbfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"/content/content/HW1-datasets/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    line = line.strip()\n",
        "    words = line.split()\n",
        "    \n",
        "    last_word = words[-1]\n",
        "\n",
        "    print(line, end=\" \")\n",
        "    \n",
        "    for _ in range(digit):\n",
        "        predicted_word = bigram_nlm.single_predict([last_word])\n",
        "\n",
        "        print(f\"\\x1b[32m{predicted_word}\\x1b[0m\", end= \" \")\n",
        "\n",
        "        last_word = predicted_word\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bu77FVgZeZz",
        "outputId": "37ccff41-f1ab-497d-ac24-2738f30158b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "این سخن حقست اگر نزد سخن گستر \u001b[32mو\u001b[0m \n",
            "این سخن حقست اگر نزد سخن گستر برند\n",
            "\n",
            "آنکه با یوسف صدیق چنین خواهد \u001b[32mو\u001b[0m \n",
            "آنکه با یوسف صدیق چنین خواهد کرد\n",
            "\n",
            "هیچ دانی چکند صحبت او با \u001b[32mتو\u001b[0m \n",
            "هیچ دانی چکند صحبت او با دگران\n",
            "\n",
            "سرمه دهی بصر بری سخت خوش است \u001b[32mو\u001b[0m \n",
            "سرمه دهی بصر بری سخت خوش است تاجری\n",
            "\n",
            "آتش ابراهیم را \u001b[32mبه\u001b[0m \u001b[32mسر\u001b[0m \n",
            "آتش ابراهیم را نبود زیان\n",
            "\n",
            "من که اندر سر \u001b[32mو\u001b[0m \u001b[32mآن\u001b[0m \n",
            "من که اندر سر جنونی داشتم\n",
            "\n",
            "هر شیر شرزه را که به نیش \u001b[32mو\u001b[0m \u001b[32mآن\u001b[0m \n",
            "هر شیر شرزه را که به نیش سنان گزید\n",
            "\n",
            "هرکه از حق به \u001b[32mسر\u001b[0m \u001b[32mو\u001b[0m \u001b[32mآن\u001b[0m \n",
            "هرکه از حق به سوی او نظریست\n",
            "\n",
            "گفت این از \u001b[32mآن\u001b[0m \u001b[32mو\u001b[0m \u001b[32mآن\u001b[0m \n",
            "گفت این از خدای باید خواست\n",
            "\n",
            "کلاه لاله که لعل است \u001b[32mو\u001b[0m \u001b[32mآن\u001b[0m \u001b[32mو\u001b[0m \n",
            "کلاه لاله که لعل است اگر تو بشناسی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trigram"
      ],
      "metadata": {
        "id": "X1RcEmtpQndF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craete and Train"
      ],
      "metadata": {
        "id": "T64j7G1WjVAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_nlm = NLM(vocabulary_size=len(vocabulary), input_size=2)"
      ],
      "metadata": {
        "id": "MxY1y2WSXxel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_nlm.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "7RaacpZkX2S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_neural_dataset(path=\"/content/content/HW1-datasets/train.txt\",\n",
        "                                         number_words=2,\n",
        "                                         line_limit=30*1000,\n",
        "                                         data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExPrRu1YX6uw",
        "outputId": "a5851c52-da9b-4a5a-bde0-a976be0e69bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 151278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, Y_valid = create_neural_dataset(path=\"/content/content/HW1-datasets/valid.txt\",\n",
        "                                         number_words=2,\n",
        "                                         data_number_limit=25*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UViZ__yBX77z",
        "outputId": "adde7eb0-ac11-4c5b-8aaa-74f29f34a5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 95190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", restore_best_weights=True, patience=2)"
      ],
      "metadata": {
        "id": "FFu6Ota9YBBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_nlm.fit(x=X_train, y=Y_train, validation_data= (X_valid, Y_valid), epochs=10, callbacks=[es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNPWafDLYDxz",
        "outputId": "34f42bf9-0ec4-4726-a467-f72ed981d9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 107s 68ms/step - loss: 7.8728 - accuracy: 0.0529 - val_loss: 6.9040 - val_accuracy: 0.0591\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 107s 69ms/step - loss: 7.0168 - accuracy: 0.0683 - val_loss: 6.9179 - val_accuracy: 0.0648\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 106s 68ms/step - loss: 6.2880 - accuracy: 0.0932 - val_loss: 7.1578 - val_accuracy: 0.0630\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 106s 68ms/step - loss: 5.2449 - accuracy: 0.1512 - val_loss: 7.6369 - val_accuracy: 0.0538\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2b9868410>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Test"
      ],
      "metadata": {
        "id": "ARGi3RrBjXEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_nlm.evaluate(x=X_train, y=Y_train, verbose=1)\n",
        "trigram_nlm.evaluate(x=X_valid, y=Y_valid, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9vx5Zm-YJI_",
        "outputId": "ad4a90ec-ca74-4aa2-8047-ae694cc5b1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 28s 18ms/step - loss: 6.3655 - accuracy: 0.0917\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 6.9179 - accuracy: 0.0648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.917877674102783, 0.0647599995136261]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train, Y_train, X_valid, Y_valid"
      ],
      "metadata": {
        "id": "-1Zxn_wBYdvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, Y_test = create_neural_dataset(path=\"/content/content/HW1-datasets/test.txt\",\n",
        "                                       number_words=2,\n",
        "                                       data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pShrzfPYfYR",
        "outputId": "6d4254e1-c071-48c8-f46b-74cae858e5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Data = 61074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_nlm.evaluate(x=X_test, y=Y_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alo58sA8Yhxg",
        "outputId": "90944974-edd2-483a-ee0d-cf808172940a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 24s 15ms/step - loss: 6.9064 - accuracy: 0.0627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.906448841094971, 0.06266000121831894]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del X_test, Y_test"
      ],
      "metadata": {
        "id": "MOrK6-0yYjL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Test"
      ],
      "metadata": {
        "id": "QoeroCnzjX8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"/content/content/HW1-datasets/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    line = line.strip()\n",
        "    words = line.split()\n",
        "    \n",
        "    \n",
        "    last_word = words[-1]\n",
        "    next_last_word = words[-2]\n",
        "\n",
        "    print(line, end=\" \")\n",
        "    \n",
        "    for _ in range(digit):\n",
        "        predicted_word = trigram_nlm.single_predict([next_last_word, last_word])\n",
        "\n",
        "        print(f\"\\x1b[32m{predicted_word}\\x1b[0m\", end= \" \")\n",
        "\n",
        "        next_last_word = last_word\n",
        "        last_word = predicted_word\n",
        "  \n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAmq7oowjfop",
        "outputId": "29706c2b-33eb-471f-b628-c795726d817a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "این سخن حقست اگر نزد سخن گستر \u001b[32mکه\u001b[0m \n",
            "این سخن حقست اگر نزد سخن گستر برند\n",
            "\n",
            "آنکه با یوسف صدیق چنین خواهد \u001b[32mدر\u001b[0m \n",
            "آنکه با یوسف صدیق چنین خواهد کرد\n",
            "\n",
            "هیچ دانی چکند صحبت او با \u001b[32mرا\u001b[0m \n",
            "هیچ دانی چکند صحبت او با دگران\n",
            "\n",
            "سرمه دهی بصر بری سخت خوش است \u001b[32mو\u001b[0m \n",
            "سرمه دهی بصر بری سخت خوش است تاجری\n",
            "\n",
            "آتش ابراهیم را \u001b[32mو\u001b[0m \u001b[32mز\u001b[0m \n",
            "آتش ابراهیم را نبود زیان\n",
            "\n",
            "من که اندر سر \u001b[32mمن\u001b[0m \u001b[32mو\u001b[0m \n",
            "من که اندر سر جنونی داشتم\n",
            "\n",
            "هر شیر شرزه را که به نیش \u001b[32mدست\u001b[0m \u001b[32mبه\u001b[0m \n",
            "هر شیر شرزه را که به نیش سنان گزید\n",
            "\n",
            "هرکه از حق به \u001b[32mو\u001b[0m \u001b[32mدست\u001b[0m \u001b[32mدل\u001b[0m \n",
            "هرکه از حق به سوی او نظریست\n",
            "\n",
            "گفت این از \u001b[32mسو\u001b[0m \u001b[32mآن\u001b[0m \u001b[32mکه\u001b[0m \n",
            "گفت این از خدای باید خواست\n",
            "\n",
            "کلاه لاله که لعل است \u001b[32mو\u001b[0m \u001b[32mو\u001b[0m \u001b[32mدل\u001b[0m \n",
            "کلاه لاله که لعل است اگر تو بشناسی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gM0kgKvND5Oc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}