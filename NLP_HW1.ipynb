{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGao0lNKPYeCLl/CLSH1O9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW1/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSRMQW5y0GbV",
        "outputId": "a432ef05-93ea-44cb-ce03-3ff371761954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
            "To: /content/HW1-datasets.zip\n",
            "100% 3.14M/3.14M [00:00<00:00, 16.2MB/s]\n",
            "Archive:  /content/HW1-datasets.zip\n",
            "replace content/HW1-datasets/train.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: content/HW1-datasets/train.txt  \n",
            "replace content/HW1-datasets/valid.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: content/HW1-datasets/valid.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete.txt  \n",
            "  inflating: content/HW1-datasets/test.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete_gold.txt  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
        "!unzip /content/HW1-datasets.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vJnzzUYZ1BoE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "vocabulary = set()\n",
        "\n",
        "for i, line in enumerate(file.readlines()):\n",
        "  if i < 30*1000:\n",
        "    words = line.split()\n",
        "\n",
        "    for word in words:\n",
        "      vocabulary.add(word)"
      ],
      "metadata": {
        "id": "hiuj1h6ZcmhY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1CUYEkEd9Sz",
        "outputId": "d5c90da5-bf99-4bde-83fb-2192daf27f26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21188"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_vocabulary = sorted(vocabulary)"
      ],
      "metadata": {
        "id": "NfZw0be4B5_t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bisect import bisect_left\n",
        "\n",
        "class BigramNLM(keras.Model):\n",
        "  def __init__(self, vocabulary):\n",
        "    super().__init__()\n",
        "    self.vocabulary = vocabulary\n",
        "    self.model = keras.models.Sequential([\n",
        "                                          keras.layers.Input((2,), name=\"Input\"),\n",
        "                                          keras.layers.Embedding(len(self.vocabulary), 128, name=\"Embedding\"),\n",
        "                                          keras.layers.Flatten(name=\"Flatten\"),\n",
        "                                          keras.layers.Dense(units=256, name=\"Hidden\"),\n",
        "                                          keras.layers.Dense(units=len(self.vocabulary), activation=\"softmax\", name=\"Output\"),\n",
        "    ])\n",
        "    \n",
        "  def convert_word_to_index(self, word):\n",
        "    pos = bisect_left(self.vocabulary, word, 0, len(self.vocabulary))\n",
        "    return pos if pos != len(self.vocabulary) and self.vocabulary[pos] == word else -1\n",
        "    # return self.vocabulary.index(word)\n",
        "\n",
        "  def convert_index_to_word(self, index):\n",
        "    return self.vocabulary[index]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    model_output =  self.model(inputs)\n",
        "    predicted_word_indexes = keras.backend.argmax(model_outputs, axis=1)\n",
        "    return predicted_word_indexes"
      ],
      "metadata": {
        "id": "e77K_UbOeJZs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnlm = BigramNLM(vocabulary=sorted_vocabulary)"
      ],
      "metadata": {
        "id": "4GmCBErDE7as"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnlm.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "CNWnHdIhSWET"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for i, line in enumerate(file.readlines()):\n",
        "  if i >= 30*1000:\n",
        "    break\n",
        "\n",
        "  words = line.split()\n",
        "\n",
        "  for j in range(2, len(words)):\n",
        "    word1 = words[j-2]\n",
        "    word2 = words[j-1]\n",
        "    word_target = words[j]\n",
        "\n",
        "    index1 = bnlm.convert_word_to_index(word1)\n",
        "    index2 = bnlm.convert_word_to_index(word2)\n",
        "    index_target = bnlm.convert_word_to_index(word_target)\n",
        "\n",
        "    X_train.append([index1, index2])\n",
        "    Y_train.append(index_target)\n",
        "\n",
        "print(len(Y_train))\n",
        "X_train = np.array(X_train[:100*1000])\n",
        "Y_train = tf.one_hot(indices=Y_train[:100*1000], depth=len(vocabulary)).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUgdUWiFJyFO",
        "outputId": "c41ba11a-5ed1-43c6-a04b-b78251bf01f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gbe0mT4SNcn",
        "outputId": "41be0305-7a14-4204-e0d3-9f1eadef4ab3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnlm.fit(x=X_train, y=Y_train, epochs=20)"
      ],
      "metadata": {
        "id": "vOfT191CNuAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           [4, 5],\n",
        "           [6, 8],\n",
        "           [2, 9],\n",
        "           ])\n",
        "\n",
        "# Y_train = [\"سلام\", \"بر\", \"ایران\",\"او\",\"از\",\"آن\",\"که\",\"آمد\",\"بر\"]\n",
        "# Y_train = [3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3]\n",
        "\n",
        "Y_train = tf.one_hot(indices=[3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3,3,2,3,4,7,5,11,2,3], depth=len(vocabulary)).numpy()\n",
        "\n",
        "bnlm.fit(x=X_train, y=Y_train, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V2KNo82NLdQ",
        "outputId": "ac6f7df0-887b-493c-d40a-69f56a4f45c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "inputs = Tensor(\"IteratorGetNext:0\", shape=(None, 2), dtype=int64)\n",
            "model_outputs = Tensor(\"bigram_nlm_3/sequential_3/Output/Softmax:0\", shape=(None, 21188), dtype=float32)\n",
            "inputs = Tensor(\"IteratorGetNext:0\", shape=(None, 2), dtype=int64)\n",
            "model_outputs = Tensor(\"bigram_nlm_3/sequential_3/Output/Softmax:0\", shape=(None, 21188), dtype=float32)\n",
            "2/2 [==============================] - 1s 154ms/step - loss: 9.9557 - accuracy: 0.0556\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 116ms/step - loss: 9.9250 - accuracy: 0.5556\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 9.8909 - accuracy: 0.5556\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 9.8511 - accuracy: 0.5556\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 9.7994 - accuracy: 0.5556\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 114ms/step - loss: 9.7332 - accuracy: 0.5556\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 110ms/step - loss: 9.6496 - accuracy: 0.5556\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 9.5409 - accuracy: 0.5556\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 9.4055 - accuracy: 0.5556\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 9.2353 - accuracy: 0.5556\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 112ms/step - loss: 9.0180 - accuracy: 0.5556\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 8.7488 - accuracy: 0.5556\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 8.4181 - accuracy: 0.5556\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 111ms/step - loss: 8.0195 - accuracy: 0.5556\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 103ms/step - loss: 7.5301 - accuracy: 0.5556\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 109ms/step - loss: 6.9398 - accuracy: 0.5556\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 6.2351 - accuracy: 0.5556\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 5.4094 - accuracy: 0.5556\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 104ms/step - loss: 4.4551 - accuracy: 0.5556\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 3.3880 - accuracy: 0.5556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c8a8f33d0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.argmax(tf.constant([\n",
        "                        [4, 5],\n",
        "                        [4, 7],\n",
        "                        [6, 2]\n",
        "                        ]), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS09XMhMPdqD",
        "outputId": "68debd4c-02f2-4215-b66c-9a1ae4da7402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 1, 0])>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DyNlj7h1NkpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}