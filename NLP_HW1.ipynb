{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNYUro6sZGpuGhJJ6YJWVyN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NLP-HW1/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "e6rp27qwNwJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "vJnzzUYZ1BoE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "18SpJaECNuHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSRMQW5y0GbV",
        "outputId": "d6b7ee66-b7cf-44a2-e96a-df6880d41586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
            "To: /content/HW1-datasets.zip\n",
            "\r  0% 0.00/3.14M [00:00<?, ?B/s]\r100% 3.14M/3.14M [00:00<00:00, 59.3MB/s]\n",
            "Archive:  /content/HW1-datasets.zip\n",
            "   creating: content/HW1-datasets/\n",
            "  inflating: content/HW1-datasets/train.txt  \n",
            "  inflating: content/HW1-datasets/valid.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete.txt  \n",
            "  inflating: content/HW1-datasets/test.txt  \n",
            "  inflating: content/HW1-datasets/test_incomplete_gold.txt  \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 16C0_9i0io43VfABV3-uukUjJYlM6k-2U\n",
        "!unzip /content/HW1-datasets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Language Model"
      ],
      "metadata": {
        "id": "ZfGWWpPEN-aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting"
      ],
      "metadata": {
        "id": "50IxVNrjOw1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "word_counter = {}\n",
        "pair_counter = {}\n",
        "vocabulary = set()\n",
        "N = 0\n",
        "\n",
        "for line in file.readlines():\n",
        "    words = line.split()\n",
        "    N += len(words)\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        vocabulary.add(word)\n",
        "        if word_counter.get(word) is None:\n",
        "            word_counter[word] = 0\n",
        "        word_counter[word] += 1\n",
        "    \n",
        "        if i>0:\n",
        "            pair = (words[i-1], word)\n",
        "            if pair_counter.get(pair) is None:\n",
        "                pair_counter[pair] = 0\n",
        "            pair_counter[pair] += 1"
      ],
      "metadata": {
        "id": "XZfxUo0POQB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "pl61UXSjO0WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram(word):\n",
        "    return word_counter[word]/N"
      ],
      "metadata": {
        "id": "HoGMfsB3OSns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bigram(sigma):\n",
        "    def calculate_B(sigma):\n",
        "        B_word = {}\n",
        "        for word1 in vocabulary:\n",
        "            B_word[word1] = len(vocabulary)\n",
        "\n",
        "        for pair in pair_counter:\n",
        "            word1 = pair[0]\n",
        "            B_word[word1] -= 1\n",
        "        \n",
        "        return B_word\n",
        "    \n",
        "    B_word = calculate_B(sigma)\n",
        "    \n",
        "    def bigram(word1, word2):\n",
        "        alpha = sigma / word_counter[word1] * B_word[word1]\n",
        "        bigram_probability_item = {}\n",
        "        p_bg = word_counter[word1] / N\n",
        "\n",
        "        return (max(pair_counter.get((word1, word2), 0) - sigma, 0) /word_counter[word1]) + alpha * p_bg\n",
        "\n",
        "    return bigram"
      ],
      "metadata": {
        "id": "JrmBTxHTOVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = create_bigram(sigma=0.1)"
      ],
      "metadata": {
        "id": "4O5V1R7iOXJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexity"
      ],
      "metadata": {
        "id": "bwlvnRgmO8px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_unigram_perplexity():\n",
        "    H = 0\n",
        "    \n",
        "    for word in vocabulary:\n",
        "#         p_word = word_counter[word]/N\n",
        "        p_word = unigram(word)\n",
        "        H -= p_word * math.log(p_word , 2)\n",
        "        \n",
        "    return 2**H\n",
        "\n",
        "def calculate_bigram_perplexity():\n",
        "    H = 0\n",
        "    \n",
        "    for i, word1 in enumerate(vocabulary):\n",
        "        if i%1000==0:\n",
        "            print(i)\n",
        "        for word2 in vocabulary:\n",
        "            p_pair = bigram(word1, word2)\n",
        "            H -= p_pair * math.log(p_pair , 2)\n",
        "        \n",
        "    return 2**H"
      ],
      "metadata": {
        "id": "ZSzVONjdO-Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_unigram_perplexity()"
      ],
      "metadata": {
        "id": "o9RPPa6_PA1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bigram_perplexity()"
      ],
      "metadata": {
        "id": "4-52CX7cPCce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Unigram"
      ],
      "metadata": {
        "id": "tAPzVoltPGku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_word = None\n",
        "best_probability = -math.inf\n",
        "for condidate_word in vocabulary:\n",
        "    if unigram(condidate_word) > best_probability:\n",
        "        best_probability = unigram(condidate_word)\n",
        "        best_word = condidate_word\n",
        "\n",
        "file = open(\"./dataset/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"./dataset/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    print(line.strip(), end=\" \")\n",
        "    for _ in range(digit):\n",
        "        print(f'\"{best_word}\"', end=\" \")\n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])    "
      ],
      "metadata": {
        "id": "B2BEpQXNPKqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Bigram"
      ],
      "metadata": {
        "id": "mEguCcW1PLz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"./dataset/test_incomplete.txt\")\n",
        "incomplete_lines = file.readlines()\n",
        "\n",
        "file = open(\"./dataset/test_incomplete_gold.txt\")\n",
        "incomplete_gold_lines = file.readlines()\n",
        "\n",
        "for i in range(len(incomplete_lines)):\n",
        "    line = incomplete_lines[i]\n",
        "    digit = int(line[0])\n",
        "    line = line[4:]\n",
        "    line = line.strip()\n",
        "    words = line.split()\n",
        "    \n",
        "    last_word = words[-1]\n",
        "    \n",
        "    print(line, end=\" \")\n",
        "    \n",
        "    for _ in range(digit):        \n",
        "        best_word = None\n",
        "        best_probability = -math.inf\n",
        "        \n",
        "        for condidate_word in vocabulary:\n",
        "            if bigram(last_word, condidate_word) > best_probability:\n",
        "                best_probability = bigram(last_word, condidate_word)\n",
        "                best_word = condidate_word\n",
        "            \n",
        "        print(f'\"{best_word}\"', end=\" \")\n",
        "        last_word = best_word\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    print(incomplete_gold_lines[i])  "
      ],
      "metadata": {
        "id": "3lhK_uBpPPzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Language Model"
      ],
      "metadata": {
        "id": "mE8eCJIqNo4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/content/HW1-datasets/train.txt\")\n",
        "\n",
        "vocabulary = set()\n",
        "\n",
        "for i, line in enumerate(file.readlines()):\n",
        "  if i < 30*1000:\n",
        "    words = line.split()\n",
        "\n",
        "    for word in words:\n",
        "      vocabulary.add(word)\n",
        "\n",
        "sorted_vocabulary = sorted(vocabulary)\n",
        "print(f\"Vocab Size = {len(vocabulary)}\")"
      ],
      "metadata": {
        "id": "hiuj1h6ZcmhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619926cc-b064-4c4a-b724-49171e1ff1b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size = 21188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bisect import bisect_left\n",
        "\n",
        "class NLM(keras.Model):\n",
        "  def __init__(self, vocabulary):\n",
        "    super().__init__()\n",
        "    self.vocabulary = vocabulary\n",
        "    self.model = keras.models.Sequential([\n",
        "                                          keras.layers.Input((2,), name=\"Input\"),\n",
        "                                          keras.layers.Embedding(len(self.vocabulary) + 1, 128, name=\"Embedding\"),\n",
        "                                          keras.layers.Flatten(name=\"Flatten\"),\n",
        "                                          keras.layers.Dense(units=256, name=\"Hidden\"),\n",
        "                                          keras.layers.Dense(units=len(self.vocabulary), activation=\"softmax\", name=\"Output\"),\n",
        "    ])\n",
        "    \n",
        "  def convert_word_to_index(self, word):\n",
        "    pos = bisect_left(self.vocabulary, word, 0, len(self.vocabulary))\n",
        "    return pos if pos != len(self.vocabulary) and self.vocabulary[pos] == word else len(self.vocabulary)\n",
        "\n",
        "  def convert_index_to_word(self, index):\n",
        "    return self.vocabulary[index]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.model(inputs)\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    model_output =  self.model(inputs)\n",
        "    predicted_word_indexes = keras.backend.argmax(model_outputs, axis=1)\n",
        "    return predicted_word_indexes"
      ],
      "metadata": {
        "id": "e77K_UbOeJZs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlm = NLM(vocabulary=sorted_vocabulary)"
      ],
      "metadata": {
        "id": "4GmCBErDE7as"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlm.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "CNWnHdIhSWET"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def create_neural_dataset(path, line_limit=None, data_number_limit=None):\n",
        "  file = open(path)\n",
        "\n",
        "  X_dataset = []\n",
        "  Y_dataset = []\n",
        "\n",
        "  for i, line in enumerate(file.readlines()):\n",
        "    if line_limit is not None and i >= line_limit:\n",
        "      break\n",
        "\n",
        "    words = line.split()\n",
        "\n",
        "    for j in range(2, len(words)):\n",
        "      word1 = words[j-2]\n",
        "      word2 = words[j-1]\n",
        "      word_target = words[j]\n",
        "\n",
        "      index1 = nlm.convert_word_to_index(word1)\n",
        "      index2 = nlm.convert_word_to_index(word2)\n",
        "      index_target = nlm.convert_word_to_index(word_target)\n",
        "\n",
        "      X_dataset.append([index1, index2])\n",
        "      Y_dataset.append(index_target)\n",
        "\n",
        "  print(len(X_dataset))\n",
        "  if data_number_limit is not None:\n",
        "    random.seed(0)\n",
        "    X_dataset, Y_dataset = zip(*random.sample(list(zip(X_dataset, Y_dataset)), data_number_limit))\n",
        "\n",
        "  X_dataset = np.array(X_dataset)\n",
        "  Y_dataset = tf.one_hot(indices=Y_dataset, depth=len(vocabulary)).numpy()\n",
        "  return X_dataset, Y_dataset"
      ],
      "metadata": {
        "id": "Lv91YrdaUEYa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, Y_valid = create_neural_dataset(path=\"/content/content/HW1-datasets/valid.txt\", data_number_limit=25*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdwYq8Vdfwuk",
        "outputId": "6f8f2dba-c946-4be7-da12-fd5b83d2a84a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = create_neural_dataset(path=\"/content/content/HW1-datasets/train.txt\",\n",
        "                                         line_limit=30*1000,\n",
        "                                         data_number_limit=50*1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igR-h-a0WtnQ",
        "outputId": "07859094-6587-4dff-d6d4-ca44f6f7ec47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlm.fit(x=X_train, y=Y_train,validation_data= (X_valid, Y_valid), epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOfT191CNuAZ",
        "outputId": "6531b8e1-b039-41c1-d317-2d11a34c59ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1563/1563 [==============================] - 199s 127ms/step - loss: 7.8775 - accuracy: 0.0526 - val_loss: 6.9137 - val_accuracy: 0.0576\n",
            "Epoch 2/4\n",
            "1563/1563 [==============================] - 201s 129ms/step - loss: 7.0093 - accuracy: 0.0675 - val_loss: 6.9126 - val_accuracy: 0.0672\n",
            "Epoch 3/4\n",
            "1563/1563 [==============================] - 201s 129ms/step - loss: 6.2552 - accuracy: 0.0926 - val_loss: 7.1538 - val_accuracy: 0.0624\n",
            "Epoch 4/4\n",
            "1563/1563 [==============================] - 195s 125ms/step - loss: 5.2125 - accuracy: 0.1514 - val_loss: 7.6302 - val_accuracy: 0.0540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f275312b410>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, Y_test = create_neural_dataset(path=\"/content/content/HW1-datasets/test.txt\")"
      ],
      "metadata": {
        "id": "Rz3vTu7IibI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}